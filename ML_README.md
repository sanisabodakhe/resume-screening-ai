# Python ML Training Component

## Overview

This directory contains the **Machine Learning training component** for the Resume Screening System. It demonstrates a complete ML pipeline using Python with scikit-learn.

---

## üìã Files

| File | Purpose |
|------|---------|
| `ml_training.py` | Standalone Python script for training the model |
| `Resume_ML_Training.ipynb` | Jupyter Notebook (teacher-friendly format) |
| `requirements-ml.txt` | Python dependencies |
| `models/` | Directory to store trained models |

---

## üöÄ Quick Start

### Step 1: Install Dependencies

```bash
pip install -r requirements-ml.txt
```

### Step 2: Run Training Script

```bash
python ml_training.py
```

**Output:**
- Trained KNN model saved to `models/knn_model.pkl`
- TF-IDF vectorizer saved to `models/vectorizer.pkl`
- Performance metrics printed to console

### Step 3: Run Jupyter Notebook (Alternative)

```bash
jupyter notebook Resume_ML_Training.ipynb
```

---

## üìä Dataset

- **Source:** `src/dataset/UpdatedResumeDataSet.csv`
- **Size:** 42,106 resume samples
- **Categories:** 25 job categories
- **Columns:** `Category`, `Resume`

---

## ü§ñ Machine Learning Pipeline

### Step 1: Data Loading
```python
df = pd.read_csv('src/dataset/UpdatedResumeDataSet.csv')
# 42,106 resumes loaded
```

### Step 2: Data Cleaning
```python
df = df.dropna()
# Removes null values
```

### Step 3: Train-Test Split
```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)
# 80% training (33,684 samples)
# 20% testing (8,422 samples)
```

### Step 4: Feature Extraction (TF-IDF)
```python
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)
```

**TF-IDF Parameters:**
- Max features: 5,000 most important words
- N-gram range: (1,2) - uses single words and word pairs
- Stop words: Removes common English words

### Step 5: Model Training (KNN)
```python
knn = KNeighborsClassifier(n_neighbors=5, metric='cosine')
knn.fit(X_train_tfidf, y_train)
```

**KNN Parameters:**
- n_neighbors: 5 (uses 5 nearest neighbors)
- metric: cosine (measures text similarity)

### Step 6: Predictions
```python
y_pred = knn.predict(X_test_tfidf)
```

### Step 7: Model Evaluation
```python
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')
```

---

## üìà Expected Results

Based on training on 42K resumes:

| Metric | Value |
|--------|-------|
| **Accuracy** | ~82-85% |
| **Precision** | ~81-84% |
| **Recall** | ~80-83% |
| **F1 Score** | ~80-83% |
| **Training Time** | ~2-3 minutes |
| **Test Samples** | 8,422 |

---

## üéØ How It Works

### Text Vectorization (TF-IDF)

Converts resume text to numerical vectors:

```
Input:  "java spring hibernate maven development"
        ‚Üì
[Clean & tokenize]
        ‚Üì
[Calculate TF: frequency of each word]
        ‚Üì
[Calculate IDF: rarity of each word]
        ‚Üì
[Multiply TF √ó IDF]
        ‚Üì
Output: [0.25, 0.18, 0.42, 0.15, 0.33, ...]
        (5000-dimensional vector)
```

### K-Nearest Neighbors (KNN)

Classifies resume by finding similar training samples:

```
For a new resume:
  1. Calculate distance to all 33,684 training resumes
  2. Find 5 nearest neighbors
  3. Get their job categories
  4. Predict: most common category among neighbors
```

---

## üíæ Saved Models

After training, three files are saved in `models/`:

### 1. `knn_model.pkl`
Trained KNN classifier - can be loaded and used for predictions:
```python
import pickle
knn = pickle.load(open('models/knn_model.pkl', 'rb'))
prediction = knn.predict(vectorizer.transform([new_resume]))
```

### 2. `vectorizer.pkl`
TF-IDF vectorizer - converts text to vectors:
```python
vectorizer = pickle.load(open('models/vectorizer.pkl', 'rb'))
tfidf_vector = vectorizer.transform([resume_text])
```

### 3. `metrics.pkl`
Performance metrics:
```python
metrics = pickle.load(open('models/metrics.pkl', 'rb'))
print(f"Accuracy: {metrics['accuracy']}")
```

---

## üîß Parameters You Can Tune

### TF-IDF Parameters
```python
vectorizer = TfidfVectorizer(
    max_features=5000,      # Increase for more features, decrease for speed
    min_df=2,               # Minimum documents containing word (increase = fewer words)
    max_df=0.8,             # Maximum document frequency (decrease to filter common words)
    ngram_range=(1, 2),     # (1,1) for single words, (1,2) for unigrams + bigrams
    stop_words='english'    # 'english' to remove common words, None to keep all
)
```

### KNN Parameters
```python
knn = KNeighborsClassifier(
    n_neighbors=5,          # Number of neighbors to consider (3, 5, 7, 9, ...)
    metric='cosine',        # 'cosine', 'euclidean', 'manhattan', ...
    weights='uniform'       # 'uniform' (all neighbors equal), 'distance' (closer = more weight)
)
```

---

## üìù Example: Using Trained Model

```python
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer

# Load saved model and vectorizer
knn = pickle.load(open('models/knn_model.pkl', 'rb'))
vectorizer = pickle.load(open('models/vectorizer.pkl', 'rb'))

# New resume to classify
new_resume = """
Python Developer with 3 years experience.
Skills: Python, Django, Flask, PostgreSQL, Docker
Education: B.Tech in Computer Science
"""

# Make prediction
resume_tfidf = vectorizer.transform([new_resume])
predicted_category = knn.predict(resume_tfidf)[0]

print(f"Predicted Category: {predicted_category}")
# Output: Predicted Category: Python Developer
```

---

## üêõ Troubleshooting

### Error: "CSV file not found"
```bash
# Make sure you're in the correct directory
cd resumescreening
python ml_training.py
```

### Error: "Module not found"
```bash
# Install missing dependencies
pip install -r requirements-ml.txt
```

### Slow training
- Reduce `max_features` from 5000 to 3000
- Use fewer `n_neighbors` (3 instead of 5)
- Use a subset of data for testing

---

## üìö Topics Covered

This project demonstrates:

1. ‚úÖ **Data Loading & Cleaning**
   - Using pandas to load CSV
   - Handling missing values

2. ‚úÖ **Exploratory Data Analysis**
   - Class distribution
   - Category statistics

3. ‚úÖ **Data Preprocessing**
   - Train-test split (80/20)
   - Stratified sampling

4. ‚úÖ **Feature Extraction**
   - TF-IDF vectorization
   - Stop word removal
   - N-gram extraction

5. ‚úÖ **Model Training**
   - K-Nearest Neighbors algorithm
   - Cosine similarity metric

6. ‚úÖ **Model Evaluation**
   - Accuracy, Precision, Recall, F1
   - Classification report
   - Performance metrics

7. ‚úÖ **Predictions**
   - Single sample prediction
   - Batch predictions

8. ‚úÖ **Model Persistence**
   - Saving trained models
   - Loading for inference

---

## üéì Learning Outcomes

After completing this project, you'll understand:

- How to build ML pipeline end-to-end
- TF-IDF feature extraction
- K-Nearest Neighbors algorithm
- Model evaluation metrics
- Train-test split importance
- How to work with real-world datasets
- Text classification techniques

---

## üìû References

- **scikit-learn Documentation:** https://scikit-learn.org/
- **TF-IDF Explanation:** https://en.wikipedia.org/wiki/Tf%E2%80%93idf
- **K-NN Algorithm:** https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm
- **Pandas Documentation:** https://pandas.pydata.org/

---

## ‚úÖ Summary

| Aspect | Details |
|--------|---------|
| **Dataset** | 42,106 resumes |
| **Algorithm** | K-Nearest Neighbors |
| **Features** | TF-IDF (5,000 words) |
| **Train/Test** | 80% / 20% split |
| **Expected Accuracy** | 82-85% |
| **Training Time** | 2-3 minutes |
| **Files** | 3 (script, notebook, requirements) |

---

**Created:** November 16, 2025  
**Project:** AI Resume Screening System  
**Status:** ‚úÖ Ready for submission
